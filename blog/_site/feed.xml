<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-08T12:25:03-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Understanding SECOND</title><link href="http://localhost:4000/2020/04/07/Understanding-SECOND.html" rel="alternate" type="text/html" title="Understanding SECOND" /><published>2020-04-07T00:00:00-04:00</published><updated>2020-04-07T00:00:00-04:00</updated><id>http://localhost:4000/2020/04/07/Understanding-SECOND</id><content type="html" xml:base="http://localhost:4000/2020/04/07/Understanding-SECOND.html">&lt;h1 id=&quot;second-sparsely-embedded-convolutional-detection&quot;&gt;SECOND (Sparsely Embedded CONvolutional Detection)&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-size:12px;&quot;&gt;by Andrew Dodd and Dean Goldman&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This blog post is the beginning of a project by the authors, involving several LiDAR detection algorithms, with a focus on &lt;a href=&quot;https://github.com/jhultman/PV-RCNN&quot;&gt;PV-RCNN&lt;/a&gt;, which utilizes SECOND.&lt;/p&gt;

&lt;p&gt;This paper is targeted towards people interested in studying and applying deep learning architectures who may not have much experience reading whitepapers published on arxiv, like SECOND. This blog post walks through the SECOND paper, and attempts to reiterate each of its components in an accessible fashion.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview:&lt;/h2&gt;

&lt;p&gt;SECOND is a deep learning architecture for object detection from point cloud data. The point cloud data comes from LiDAR (Light Detection And Ranging), a method of measuring light that has been pulsed outwards from a laser at a central point, reflected off the world, and bounced back into a detector at the initial central point. The SECOND model is trained on the KITTI Vision Benchmark Suite, a common open source LiDAR dataset. Each point cloud space describes a 3D space generally depicting a road with cars, cyclists, and pedestrians. It has an associated set of labels:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The bounding boxes of objects in the space.
    &lt;ul&gt;
      &lt;li&gt;The x, y, z coordinates of the object’s center.&lt;/li&gt;
      &lt;li&gt;The length (l), width (w), and depth (d) of the bounding box.&lt;/li&gt;
      &lt;li&gt;The yaw (z-axis rotation) of the bounding box.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The classification of each object in the space
    &lt;ul&gt;
      &lt;li&gt;One-hot encoded variable for car, cyclist, pedestrian (or more, depending on dataset used)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Each object’s orientation in space.
    &lt;ul&gt;
      &lt;li&gt;One-hot encoded variable describing the orientation of the box: Is the front of the box facing away or towards the LiDAR’s central point?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SECOND’s proposed technique takes a point cloud and goes through the following steps (each described briefly below, more detail to follow):&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Point cloud Grouping&lt;/strong&gt;: group points into voxels (voxels can be thought of as spatial 3D grid data)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Voxel Feature Extraction&lt;/strong&gt;: process the points in each voxel to create overall voxel features&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sparse Convolutional Middle Extractor&lt;/strong&gt;: sequence of convolutional blocks (sparse convolutions are well optimized convolutions for spread out data, like car point clouds!)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Region Proposal Network (RPN)&lt;/strong&gt;: more convolutional blocks to get the desired output shape&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Proposal Layer&lt;/strong&gt;: dense layers to predict the above labels&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Loss&lt;/strong&gt;: Calculate classification, bounding box, direction losses&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: 
The dimensions of the inputs and outputs of this network are based off of the implementation in the &lt;a href=&quot;https://github.com/jhultman/PV-RCNN&quot;&gt;PV-RCNN&lt;/a&gt; codebase by &lt;a href=&quot;https://github.com/jhultman&quot;&gt;jhultman&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The network is illustrated in the SECOND paper like so:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Understanding_SECOND/SECOND_network.png&quot; alt=&quot;SECOND Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This blog post describes each of the network layers’ Input, Output, and a description of the layers’ algorithm.&lt;/p&gt;

&lt;h2 id=&quot;1-point-cloud-grouping&quot;&gt;1. Point Cloud Grouping&lt;/h2&gt;

&lt;h3 id=&quot;input&quot;&gt;Input:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Points&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm:&lt;/h3&gt;
&lt;p&gt;Given a set size of an individual voxel, iterate through the 3d space, with each step being a new voxel, and group the points within each voxel space into a tensor of points per voxel.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Voxel size: Size of individual voxel&lt;/li&gt;
  &lt;li&gt;Grid bounds: Boundary limit for points to include in voxel space&lt;/li&gt;
  &lt;li&gt;Max voxels: Maximum number of voxels allowed in algorithm&lt;/li&gt;
  &lt;li&gt;Max occupancy: Limit of points to include in a single voxel&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;points&quot;&gt;Points&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Voxels&lt;/li&gt;
  &lt;li&gt;Matrix of points per voxel&lt;/li&gt;
  &lt;li&gt;Coordinate and number-of-voxels map&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each point has an x,y,z coordinate and a reflecitivy value r.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
x_{1} &amp; y_{1} &amp; z_{1} &amp; r_{1}\\
...\\
x_{p} &amp; y_{p} &amp; z_{p} &amp; r_{p}\\
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;p is the total number of points in the point cloud.&lt;/p&gt;

&lt;h4 id=&quot;voxels&quot;&gt;Voxels&lt;/h4&gt;
&lt;p&gt;Each voxel is of dimension &lt;script type=&quot;math/tex&quot;&gt;( Point Features * Max Occupancy )&lt;/script&gt;&lt;br /&gt;
In this case each point has four dimensions: an x, y, z, and reflectivity (r). Letting the maximum occupancy per voxel = 5, we’d have the following matrix representation of a voxel.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
x_{1} &amp; y_{1} &amp; z_{1} &amp; r_{1}\\
x_{2} &amp; y_{2} &amp; z_{2} &amp; r_{2}\\
x_{3} &amp; y_{3} &amp; z_{3} &amp; r_{3}\\
x_{4} &amp; y_{4} &amp; z_{4} &amp; r_{4}\\
x_{5} &amp; y_{5} &amp; z_{5} &amp; r_{5}\\
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since generally, point cloud data aren’t that closely clustered compared to the number of voxels being used, the voxels will generally have just one or two points in them.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
x_{1} &amp; y_{1} &amp; z_{1} &amp; r_{1}\\
0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0\\
0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0\\
0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0\\
0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0\\
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;This step also requires a data structure to hold the spatial coordinates and corresponding number of points per voxel. This map will be length &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, a set maximum number of voxels.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{|c|c|c|}
\hline
  \text{Index} &amp; \text{Coordinates} &amp; \text{Num Points in Voxel} \\ 
\hline  
   0 &amp; (x_{0}, y_{0}, z_{0}) &amp; 1 \\ 
\hline
   1 &amp; (x_{1}, y_{1}, z_{1}) &amp; 0 \\ 
\hline
   2 &amp; (x_{2}, y_{2}, z_{2}) &amp; 1 \\ 
\hline
   3 &amp; (x_{3}, y_{3}, z_{3}) &amp; 2 \\ 
\hline
   ... &amp; ... &amp; ... \\ 
\hline
   n &amp; (x_{n}, y_{n}, z_{n}) &amp; 1 \\ 
\hline
\end{array} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;2-voxelwise-feature-extractor-vfe&quot;&gt;2. Voxelwise Feature Extractor (VFE)&lt;/h2&gt;
&lt;h3 id=&quot;input-1&quot;&gt;Input:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Voxels with points&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;output&quot;&gt;Output:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Point-wise + voxel-wise feature extractions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm-1&quot;&gt;Algorithm:&lt;/h3&gt;

&lt;p&gt;A Fully Connected Layer (FCN) consisting of a Linear layer, Batch Normalization and ReLU layer extract point-wise features. It then uses voxel-wise max pooling to obtain an aggregated set of features per voxel. This is concatenated to the point-wise features. The final output of the FCN is a sparse 4D tensor:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
C_{0} &amp; D_{0} &amp; H_{0} &amp; W_{0}\\
&amp; ...\\
C_{v} &amp; D_{v} &amp; H_{v} &amp; W_{v}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;C_{v}&lt;/script&gt; is the vector of point-wise + locally aggregated feature extraction.&lt;/p&gt;

&lt;p&gt;A Linear layer takes as input a tensor (in this case the points per voxel) and applies a linear transformation (&lt;script type=&quot;math/tex&quot;&gt;y=xA^{T}+b&lt;/script&gt;). This is equivalent to a matrix 
multiplication where the dimensions are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[V, P, V_{in}] * [V_{in}, U] = [V, P, U]&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;: Total number of voxels&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;: Total number of points&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;V_{in}&lt;/script&gt;: A vector of x,y,z coordinates and r per point, along with each coordinate relative offset w.r.t. the centroid of points in the voxel space, v. (This is just computing the mean of all point coordinates per voxel.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[x_{i}, y_{i}, z_{i}, r_{i}, x_{i}-v_{i}, y_{i}-v_{i}, z_{i}-v_{i}]&lt;/script&gt;

&lt;h2 id=&quot;3-sparse-convolutional-middle-extractor&quot;&gt;3. Sparse Convolutional Middle Extractor&lt;/h2&gt;
&lt;h3 id=&quot;input-2&quot;&gt;Input:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Voxel Feature Extractions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;output-1&quot;&gt;Output:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Features from Middle Extractor: &lt;script type=&quot;math/tex&quot;&gt;(n, H, W, (C * D))&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm-2&quot;&gt;Algorithm:&lt;/h3&gt;
&lt;p&gt;This layer is a network of convolutional blocks for 3D tensors. The initial block is of dimension &lt;script type=&quot;math/tex&quot;&gt;(11, 400, 352)&lt;/script&gt; and represents the total number of potential voxels within the voxel space of the LiDAR detection (&lt;script type=&quot;math/tex&quot;&gt;11 * 400 * 352 = 1,548,800&lt;/script&gt;). Out of the ~ 1.55 million voxels, there are relatively very few non-empty voxels. This is what characterizes the network as “sparse”.&lt;/p&gt;

&lt;p&gt;As the voxels and their features enter the convolution block, they are reshaped by the convolutional filter roughly following this pattern.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Understanding_SECOND/SECOND_middle.png&quot; alt=&quot;SECOND Middle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The pink box in the diagram is an enlarged representation of a single voxel, we’ll assume its non-empty. For that pink box, it contains &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, that voxel’s feature extraction from the VFE layer. The middle extractor collapses the Z-dimension down to eventually form a birds-eye-view of the space, resulting in a shape &lt;script type=&quot;math/tex&quot;&gt;(n, H, W, (C * D))&lt;/script&gt;
where n is the batch size, and &lt;script type=&quot;math/tex&quot;&gt;C * D&lt;/script&gt; is size of the new voxel features within the downsampled z-dimension.&lt;/p&gt;

&lt;h2 id=&quot;4-regional-proposal-network-rpn&quot;&gt;4. Regional Proposal Network (RPN)&lt;/h2&gt;
&lt;h3 id=&quot;input-3&quot;&gt;Input:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Features from Convolution Middle Layer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;output-2&quot;&gt;Output:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Region Proposal Features (same shape as Convolutional Middle layer Output)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm-3&quot;&gt;Algorithm:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The RPN sends the BEV feature maps from the Middle Layer into a sequence of three 2D convolutional blocks. Each block starts with a stride-2 downsampling layer, a number of stride-1 layers in between and a &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic&quot;&gt;transpose convolutional layer&lt;/a&gt; at the end, that upsamples the tensor shape to its original dimensions. After each block, the resulting feature map is concatenated to a final feature map, and then sent into the next block.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-proposal-layer&quot;&gt;5. Proposal Layer&lt;/h2&gt;
&lt;h3 id=&quot;input-4&quot;&gt;Input:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Region Proposal Features&lt;/li&gt;
  &lt;li&gt;Anchors&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;output-3&quot;&gt;Output:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Category Classification&lt;/li&gt;
  &lt;li&gt;Box Regression&lt;/li&gt;
  &lt;li&gt;Direction Classification&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm-4&quot;&gt;Algorithm:&lt;/h3&gt;
&lt;p&gt;…&lt;/p&gt;

&lt;h2 id=&quot;6-loss&quot;&gt;6. Loss&lt;/h2&gt;
&lt;h3 id=&quot;input-5&quot;&gt;Input:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Category Classification&lt;/li&gt;
  &lt;li&gt;Box Regression&lt;/li&gt;
  &lt;li&gt;Direction Classification&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;output-4&quot;&gt;Output:&lt;/h3&gt;
&lt;p&gt;A combined loss for the three prediction categories.&lt;/p&gt;

&lt;h3 id=&quot;algorithm-5&quot;&gt;Algorithm:&lt;/h3&gt;
&lt;h4 id=&quot;category-classification-loss&quot;&gt;Category Classification Loss:&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FL(p_{t}) = -\alpha(1-p_{t})^{\gamma}log(1-p_{t})&lt;/script&gt;

&lt;h4 id=&quot;box-regression-loss&quot;&gt;Box Regression Loss:&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{\theta} = SmoothL1(sin(\theta_{p} - \theta_{t}))&lt;/script&gt;

&lt;p&gt;The SmoothL1 loss function (a.k.a Huber Loss) is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
L_{\sigma}(a) = \begin{cases}
  \frac{1}{2}a^{2} &amp; \text{if} for |a| &lt;= \sigma\\
  \sigma(|a|-\frac{1}{2}\sigma) &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is the residual, in this case: &lt;script type=&quot;math/tex&quot;&gt;a = sin(\theta_{p} - \theta_{t})&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;direction-classification-loss&quot;&gt;Direction Classification Loss:&lt;/h4&gt;
&lt;p&gt;Softmax function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(y_{i}) = \frac{e^{y_{i}}}{\sum_{j=0}^n e^{y_{j}}}&lt;/script&gt;

&lt;p&gt;The Softmax function takes in a vector of length &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; of numbers or “logits” and outputs a vector of length &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; that sums to 1. In this case, &lt;script type=&quot;math/tex&quot;&gt;n=2&lt;/script&gt;, to classify if the detected object is front-facing or back-facing.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To recap, this network is a neural network used to read a set of LiDAR points to predict object locations (generally car, pedestrian, bike). This network’s general process to achieve this is grouping the points to voxels, performing VFE (making features for each voxel), passing this through some 3D convolutions (sparse feature extractor), then going through another set of convolutions (RPN) to generate classifications and bounding box proposals. From there, it can calculate loss, backpropagate and update its parameters.&lt;/p&gt;

&lt;p&gt;Things this blog post does not deeply dive into:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Implementational details&lt;/li&gt;
  &lt;li&gt;Why each specific layer was used&lt;/li&gt;
  &lt;li&gt;Performance metrics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We may update this post to include more details, but for now, we hope this is a good primer before reading the paper :)&lt;/p&gt;

&lt;p&gt;Thanks for reading our very first blog post! Feel free to contact the authors with comments, questions, concerns, etc. We are eager to hear from our readers.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;p&gt;Yan, Y.; Mao, Y; Li, B. SECOND: Sparsely Embedded Convolutional Detection. Sensors, vol. 18, no. 10, June 2018, p. 3337.&lt;/p&gt;

&lt;p&gt;Vasudevan, A.; Anderson, A.; Gregg, D. Parallel multi channel convolution using general matrix
multiplication. In Proceedings of the 2017 IEEE 28th International Conference on Application-specific
Systems, Architectures and Processors (ASAP), Seattle, WA, USA, 10–12 July 2017; pp. 19–24.&lt;/p&gt;

&lt;p&gt;Zhou, Y.; Tuzel, O. VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. arXiv 2017,
arXiv:1711.06396.&lt;/p&gt;</content><author><name></name></author><summary type="html">SECOND (Sparsely Embedded CONvolutional Detection) by Andrew Dodd and Dean Goldman</summary></entry><entry><title type="html">Lattice</title><link href="http://localhost:4000/2020/04/01/lattice.html" rel="alternate" type="text/html" title="Lattice" /><published>2020-04-01T00:00:00-04:00</published><updated>2020-04-01T00:00:00-04:00</updated><id>http://localhost:4000/2020/04/01/lattice</id><content type="html" xml:base="http://localhost:4000/2020/04/01/lattice.html">&lt;script type=&quot;text/tikz&quot;&gt;

% sparse voxel grid with blue active voxels
\begin{tikzpicture}
% starting at lower left, slope of face down, repetition angle up
\begin{scope}[every node/.append style={yslant=-.5},yslant=-.5]
  \draw[opacity=0.5, scale=0.5] (10, 10 ) grid ( 0, 0  );
  \draw[opacity=0.1, scale=0.5] (11, 11 ) grid ( 1, 1  );
  \draw[opacity=0.1, scale=0.5] (12, 12 ) grid ( 2, 2  );
  \draw[opacity=0.1, scale=0.5] (13, 13 ) grid ( 3, 3  );
  \draw[opacity=0.1, scale=0.5] (14, 14 ) grid ( 4, 4  );
  \draw[opacity=0.1, scale=0.5] (15, 15 ) grid ( 5, 5  );
  \draw[opacity=0.1, scale=0.5] (16, 16 ) grid ( 6, 6  );
  \draw[opacity=0.1, scale=0.5] (17, 17 ) grid ( 7, 7  );
  \draw[opacity=0.1, scale=0.5] (18, 18 ) grid ( 8, 8  );
  \draw[opacity=0.1, scale=0.5] (19, 19 ) grid ( 9, 9  );
  \draw[opacity=0.1, scale=0.5] (20, 20 ) grid ( 10, 10  ); 
  % \node at (0,0)[color=red!100] {X};
\end{scope}
% starting at lower left, slope of face up, repetition angle down
\begin{scope}[every node/.append style={yslant=-.5},yslant=-.5]
  \draw[opacity=0.1, scale=0.5] (10, 10 ) grid ( 20, 10  );
  \draw[opacity=0.1, scale=0.5] ( 9, 9  ) grid ( 19, 9  );
  \draw[opacity=0.1, scale=0.5] ( 8, 8  ) grid ( 18, 8  );
  \draw[opacity=0.1, scale=0.5] ( 7, 7  ) grid ( 17, 7  );
  \draw[opacity=0.1, scale=0.5] ( 6, 6  ) grid ( 16, 6  );
  \draw[opacity=0.1, scale=0.5] ( 5, 5  ) grid ( 15, 5  );
  \draw[opacity=0.1, scale=0.5] ( 4, 4  ) grid ( 14, 4  );
  \draw[opacity=0.1, scale=0.5] ( 3, 3  ) grid ( 13, 3  );
  \draw[opacity=0.1, scale=0.5] ( 2, 2  ) grid ( 12, 2  );
  \draw[opacity=0.1, scale=0.5] ( 1, 1  ) grid ( 11, 1  );
  \draw[opacity=0.1, scale=0.5] ( 0, 0  ) grid ( 10, 0 );
  % \node at (0,0)[color=green!100] {X};
\end{scope}

 \begin{scope}[every node/.append style={yslant=0.5},yslant=0.5]
   \draw[opacity=0.5, scale=0.5] (10, -10 ) grid ( 20, 0  );
   \draw[opacity=0.1, scale=0.5] ( 9, -9  ) grid ( 19, 1  );
   \draw[opacity=0.1, scale=0.5] ( 8, -8  ) grid ( 18, 2  );
   \draw[opacity=0.1, scale=0.5] ( 7, -7  ) grid ( 17, 3  );
   \draw[opacity=0.1, scale=0.5] ( 6, -6  ) grid ( 16, 4  );
   \draw[opacity=0.1, scale=0.5] ( 5, -5  ) grid ( 15, 5  );
   \draw[opacity=0.1, scale=0.5] ( 4, -4  ) grid ( 14, 6  );
   \draw[opacity=0.1, scale=0.5] ( 3, -3  ) grid ( 13, 7  );
   \draw[opacity=0.1, scale=0.5] ( 2, -2  ) grid ( 12, 8  );
   \draw[opacity=0.1, scale=0.5] ( 1, -1  ) grid ( 11, 9  );
   \draw[opacity=0.1, scale=0.5] ( 0, -0  ) grid ( 10, 10 );

  % kernel
  \fill[red,fill opacity=.1, scale=0.5] ( 7, 3 ) rectangle ( 10, 0 );
  \fill[red,fill opacity=.2, scale=0.5] ( 10, 0 ) rectangle ( 13, -3 ); 

  % top corner cube
  \fill[blue,fill opacity=.3, scale=0.5] ( 9, 1 ) rectangle ( 10, 0 );
  \fill[blue,fill opacity=.5, scale=0.5] ( 11, 0 ) rectangle ( 10, -1 );

  % lower left cube
  \fill[blue,fill opacity=.1, scale=0.5] ( 5, 1 ) rectangle ( 4, 0 );
  \fill[blue,fill opacity=.1, scale=0.5] ( 6, 0 ) rectangle ( 5, -1 );

  % behind top corner cube
  \fill[blue,fill opacity=.1, scale=0.5] ( 9, 6 ) rectangle ( 8, 5 );
  \fill[blue,fill opacity=.1, scale=0.5] ( 10, 5 ) rectangle ( 9, 4 );
  % \node at (0,0)[color=green!100, scale=0.5] {X};
  
\end{scope}

% moving downwards
\begin{scope}[every node/.append style={
    yslant=0.5,xslant=-1},yslant=0.5,xslant=-1
  ]
  \draw[opacity=0.1, scale=0.5] (10, 10 ) grid ( 20,  0  );
  \draw[opacity=0.1, scale=0.5] ( 9, 9  ) grid ( 19, -1  );
  \draw[opacity=0.1, scale=0.5] ( 8, 8  ) grid ( 18, -2  );
  \draw[opacity=0.1, scale=0.5] ( 7, 7  ) grid ( 17, -3  );
  \draw[opacity=0.1, scale=0.5] ( 6, 6  ) grid ( 16, -4  );
  \draw[opacity=0.1, scale=0.5] ( 5, 5  ) grid ( 15, -5  );
  \draw[opacity=0.1, scale=0.5] ( 4, 4  ) grid ( 14, -6  );
  \draw[opacity=0.1, scale=0.5] ( 3, 3  ) grid ( 13, -7  );
  \draw[opacity=0.1, scale=0.5] ( 2, 2  ) grid ( 12, -8  );
  \draw[opacity=0.1, scale=0.5] ( 1, 1  ) grid ( 11, -9  );
  \draw[opacity=0.1, scale=0.5] ( 0, 0  ) grid ( 10, -10 );

  % kernel
  \fill[red,fill opacity=.1, scale=0.5] ( 13, 3 ) rectangle ( 10, 0 ); 
  \fill[red,fill opacity=.1, scale=0.5] ( 10, 0 ) rectangle ( 7, -3 );

  % top corner cube
  \fill[blue,fill opacity=.5, scale=0.5] ( 10, 0 ) rectangle ( 9, -1 );
  \fill[blue,fill opacity=.3, scale=0.5] ( 11, 1 ) rectangle ( 10, 0 );

  % lower left cube
  \fill[blue,fill opacity=.1, scale=0.5] ( 5, 1 ) rectangle ( 6, 0 );
  \fill[blue,fill opacity=.1, scale=0.5] ( 4, 0 ) rectangle ( 5, -1 );
  
  % behind top corner cube
  \fill[blue,fill opacity=.1, scale=0.5] ( 15, 6 ) rectangle ( 14, 5 );
  \fill[blue,fill opacity=.1, scale=0.5] ( 14, 5 ) rectangle ( 13, 4 );
  % \node at (0,0)[color=green!100, scale=0.5] {X}; % help marker
\end{scope}

% annotation
\draw[-latex,thick,black](-1,-2) node[text width=7cm]
{
3x3x3 kernel convolving over voxel space
$\mathsf{(D_{i}, W_{i}, H_{i} ... D_{i+3}, W_{i+3}, H_{i+3})}$
(where i corresponds to the ith voxel in the voxel space)
 for a single VFE feature (j)
Where each voxel holds $\mathsf{C_{j}}$,
the jth feature of the VFE layer output.
}
to[out=90,in=180] (4,2.5);


\end{tikzpicture}



&lt;/script&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Bayesian Inference</title><link href="http://localhost:4000/2020/03/20/Bayesian-Inference.html" rel="alternate" type="text/html" title="Bayesian Inference" /><published>2020-03-20T00:00:00-04:00</published><updated>2020-03-20T00:00:00-04:00</updated><id>http://localhost:4000/2020/03/20/Bayesian-Inference</id><content type="html" xml:base="http://localhost:4000/2020/03/20/Bayesian-Inference.html">&lt;p&gt;Bayesian Inference derives the posterior probability of an event as a consequence of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A prior probability: A distribution of probabilty about an outcome priort to some evidence, denoted as &lt;script type=&quot;math/tex&quot;&gt;P(H)&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;P(H)&lt;/script&gt; is the probability that park will be crowded when I go there today. It is the probability &lt;em&gt;prior&lt;/em&gt; to knowing anything about evidence that may inform my probability distribution (weather, day of week, time of year, current events, etc.).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A likelihood function: A function describing the probability of an event E occuring, for a fixed probability of H occuring: &lt;script type=&quot;math/tex&quot;&gt;P(E \vert H)&lt;/script&gt;. What is the probability that it was a nice day &lt;em&gt;given&lt;/em&gt; that the park was crowded when I went there?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model evidence: The probability of seeing evidence E: &lt;script type=&quot;math/tex&quot;&gt;P(E)&lt;/script&gt;. What is the probability that it will be a nice day?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This will yield the posterior probability:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(H \vert E) = \frac{P(E \vert H) * P(H)}{P(E)}&lt;/script&gt;

&lt;p&gt;Which gives us the probability of the hypothsis (&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;) given evidence &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;. What is the probability that the park will be crowded &lt;em&gt;given&lt;/em&gt; that it is a nice day?&lt;/p&gt;

&lt;h2 id=&quot;run-the-numbers&quot;&gt;Run the numbers:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Lets start with the prior probability that the park will be crowded when I go there. Deciding on a probability seems like it should always be &lt;script type=&quot;math/tex&quot;&gt;50\%&lt;/script&gt;, its either a “yes” or a “no”, but it doesn’t have to be!e
This example has a property in common with many priors, namely, that the posterior from one problem (today’s temperature) becomes the prior for another problem (tomorrow’s temperature); pre-existing evidence which has already been taken into account is part of the prior and, as more evidence accumulates, the posterior is determined largely by the evidence rather than any original assumption, provided that the original assumption admitted the possibility of what the evidence is suggesting. The terms “prior” and “posterior” are generally relative to a specific datum or observation.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Bayesian Inference derives the posterior probability of an event as a consequence of:</summary></entry></feed>